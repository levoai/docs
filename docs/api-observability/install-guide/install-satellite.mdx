---
sidebar_position: 3
---

# Install Satellite

## 1. Prerequisites
- You have an account on [Levo.ai](https://app.levo.ai/login)
- [OS Compatibility script](./os-compat-check.mdx) indicates the Linux host (that you want to instrument with the Sensor) is compatible.
- At least 4 CPUs
- At least 8 GB RAM

## 2. Planning

### a. Placement of the Satellite
There are two options for the Satellite placement. Choose what best works for you.

| Placement Type              | Pros                | Cons                |
| :---------------------------| :----------         | :-------------      |
| Same Host/Cluster as Sensor | <ul><li> Simpler installation.</li> <li>Easy to get started.</li></ul> | Satellite consumes resources on the host/cluster where your application workloads are located. <br></br> This might lead to resource contention based on traffic load. |
| Dedicated Host/Cluster      | <ul><li> Eliminates any resource contention issues with your application workloads.</li> <li> The Satellite can service traffic from multiple hosts/clusters that are instrumented.</li></ul> | Requires a dedicated host/cluster. |

### b. Copy `Authorization Key` from Levo.ai
The Satellite uses an authorization key to access Levo.ai. Follow instructions below to copy the key.
- [Login](https://app.levo.ai/login) to Levo.ai.
- Click on your user profile.
- Click on `User Settings`
- Click on `Keys` on the left navigation panel
- Click on `Get Satellite Authorization Key`
- Now copy your authorization key. This key is required in a subsequent step below.


## 3. Follow instructions for your platform
- [Install on Kubernetes](#install-on-kubernetes)
- [Install on Linux host via Docker Compose](#install-on-linux-host-via-docker-compose)
- [Install in AWS using Levo Satellite AMI](#install-in-aws-ec2-using-levo-satellite-ami)
- [Install in AWS EKS using EC2](#install-in-aws-eks-using-ec2)

<br></br>

-------------------------------------------------

## Install on Kubernetes

### Prerequisites
- Kubernetes version >= `v1.18.0`
- [Helm v3](https://helm.sh/docs/intro/install/) installed and working.
- The Kubernetes cluster API endpoint should be reachable from the machine you are running Helm.
- `kubectl` access to the cluster, with `cluster-admin` permissions.
- At least 4 CPUs
- At least 8 GB RAM

### 1. Setup environment variables

```bash
export LEVOAI_AUTH_KEY=<'Authorization Key' copied earlier> 
```

### 2. Install levoai Helm repo
```bash
helm repo add levoai https://charts.levo.ai && helm repo update
```

### 3. Create `levoai` namespace & install Satellite

#### If locating Satellite on the same cluster alongside Sensor
```bash
helm upgrade --install -n levoai --create-namespace \
  --set global.levoai_config_override.onprem-api.refresh-token=$LEVOAI_AUTH_KEY \
  levoai-satellite levoai/levoai-satellite --force
```

#### If locating Satellite on a dedicated cluster
You will need to expose the Satellite via either a `LoadBalancer` or `NodePort`, such that is is reachable by Sensors running in other clusters. Please modify the below command appropriately.

```bash
# Please modify this command template and choose either 'LoadBalancer' or 'NodePort', prior to execution
helm upgrade --install -n levoai --create-namespace \
    --set global.levoai_config_override.onprem-api.refresh-token=$LEVOAI_AUTH_KEY \
    --set levoai-collector.service.type=<LoadBalancer | NodePort> \
    levoai-satellite levoai/levoai-satellite --force
```

### 4. Verify connectivity with Levo.ai

#### a. Check Satellite health

The Satellite is comprised of five sub components 1) levoai-collector, 2) levoai-ion, 3) levoai-rabbitmq, 4)levoai-satellite, and 5) levoai-tagger.

Wait couple of minutes after the install, and check the health of the components by executing the following:

```bash
kubectl -n levoai get pods
```                              
If the Satellite is healthy, you should see output similar to below. Don't worry about the restarts of the levoai-tagger pod.

```bash
NAME                                READY   STATUS    RESTARTS      AGE
levoai-collector-848fb4fff9-gv8g9   1/1     Running   0             4m8s
levoai-ion-65686bd9c6-k2vgc         1/1     Running   0             4m8s
levoai-rabbitmq-0                   1/1     Running   0             4m8s
levoai-satellite-54956ccb89-5s4h2   1/1     Running   0             4m8s
levoai-tagger-799db4d9cc-89jm8      1/1     Running   3 (4m8s ago)  4m8s
```


#### b. Check connectivity
Execute the following to check for connectivity health:

```bash
# Please specify the actual pod name for levoai-tagger below
kubectl -n levoai logs <levoai-tagger pod name> | grep "Ready to process; waiting for messages."
```
If connectivity is healthy, you will see output similar to below.

```bash
{"level": "info", "time": "2022-06-07 08:07:22,439", "line": "rabbitmq_client.py:155", "version": "fc628b50354bf94e544eef46751d44945a2c55bc", "module": "/opt/levoai/e7s/src/python/levoai_e7s/satellite/rabbitmq_client.py", "message": "Ready to process; waiting for messages."}
```

**Please contact `support@levo.ai` if you notice health/connectivity related errors.**

### 5. Note down `Host:Port` information

#### If locating Satellite on the same cluster alongside Sensor
The Collector can now be reached by the Sensors running in the same cluster at `levoai-collector.levoai:4317`. Please note this, as it will be required to configure the Sensor.

#### If locating Satellite on a dedicated cluster
Run the below command and note the `external` address/port of the the Collector service. This will be required to configure the Sensor.

```bash
kubectl get service levoai-collector -n levoai
```
Please proceed to install the Sensor.

<br></br>

-------------------------------------------------

## Install on Linux host via Docker Compose

import BrowserOnly from '@docusaurus/BrowserOnly';

export function DownloadDockerComposeLink() {
  return (
    <BrowserOnly fallback={<div>Loading...</div>}>
      {() => (
        <a href={window.location.protocol + '//' + window.location.host + '/artifacts/satellite/docker-compose.yml'} download> Download</a>
      )}
    </BrowserOnly>
  );
}

### Prerequisites
- Docker Engine version `18.03.0` and above
- Admin privileges on the Docker host
- 'docker-compose' installed, if 'docker compose' is not supported on your OS
- At least 4 CPUs
- At least 8 GB RAM

### 1. Download Docker Compose file
Levo provides pre-built Docker images for the Satellite that can be installed via Docker Compose.
<DownloadDockerComposeLink/> the Docker Compose file to your desktop.

### 2. Install Satellite
Execute the following from the directory where the Docker Compose file was downloaded.

```bash
(export LEVOAI_AUTH_KEY=<'Authorization Key' copied earlier>; docker compose pull && docker compose up -d)
```

> If `docker compose ...` complains with *"docker: 'compose' is not a docker command."*, you have can try **`docker-compose`** instead.

### 3. Verify connectivity with Levo.ai

#### a. Check Satellite health

The Satellite is comprised of four sub components 1) levoai-collector, 2) levoai-rabbitmq, 3)levoai-satellite, and 4) levoai-tagger.

Wait couple of minutes after the install, and check the health of the components by executing the following:

```bash
docker ps -f name=levoai
```

If the Satellite is healthy, you should see output similar to below.

```bash
CONTAINER ID   IMAGE                     COMMAND                  CREATED             STATUS                  PORTS                                                                                                                                    NAMES
2b32cd6b9ced   levoai/collector:stable   "/usr/local/bin/levo…"   10 seconds ago      Up 8 seconds            0.0.0.0:4317->4317/tcp, 9411/tcp                                                                                                         levoai-collector
06f3c597cad0   levoai/satellite:stable   "gunicorn --capture-…"   10 seconds ago      Up 9 seconds            0.0.0.0:9999->9999/tcp                                                                                                                   levoai-satellite
89026034c567   levoai/satellite:stable   "python -OO /opt/lev…"   10 seconds ago      Up Less than a second                                                                                                                                            levoai-tagger
f74524d02fbd   bitnami/rabbitmq:3.10     "/opt/bitnami/script…"   10 seconds ago      Up 9 seconds            5551-5552/tcp, 0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 15671/tcp   levoai-rabbitmq
```

#### b. Check connectivity
Execute the following to check for connectivity health:

```bash
docker logs levoai-tagger | grep "Ready to process; waiting for messages." 
```
If connectivity is healthy, you will see output similar to below.

```bash
{"level": "info", "time": "2022-06-07 08:07:22,439", "line": "rabbitmq_client.py:155", "version": "fc628b50354bf94e544eef46751d44945a2c55bc", "module": "/opt/levoai/e7s/src/python/levoai_e7s/satellite/rabbitmq_client.py", "message": "Ready to process; waiting for messages."}
```

### 4. Note down `Host:Port` information
The Collector now runs in a container, and is reachable on the host via port 4317 (on all the host's network interfaces).

Please note down the either the host's IP address or domain name. The Sensor will be configured to communicate with the Collector at <Host's IP|Domain-Name>:4317.

Please proceed to install the Sensor.

<br></br>

-------------------------------------------------

## Install in AWS EC2 using Levo Satellite AMI

### 1. Open the EC2 Launch Wizard and select the Levo Satellite AMI
Levo provides pre-built AMIs for Satellite. You can launch an EC2 instance using the AMI in the AWS region you wish to install the satellite in.

<Tabs groupId="satellite-ami">
  <TabItem value="mac" label="Mac OSX">
    <ul>
      <li> <a href="https://us-west-2.console.aws.amazon.com/ec2/home?region=us-west-1#LaunchInstances:ami=ami-0f21c115fef0a81ac;instanceType=c6a.xlarge">us-west-1 (N. California)</a> </li>
      <li> <a href="https://us-west-2.console.aws.amazon.com/ec2/home?region=us-west-2#LaunchInstances:ami=ami-005736ee5dd0fa660;instanceType=c6a.xlarge">us-west-2 (Oregon)</a> </li>
      <li> <a href="https://us-west-2.console.aws.amazon.com/ec2/home?region=us-east-1#LaunchInstances:ami=ami-052270e3748e0cb0a;instanceType=c6a.xlarge">us-east-1 (N. Virginia)</a> </li>
      <li> <a href="https://us-west-2.console.aws.amazon.com/ec2/home?region=us-east-2#LaunchInstances:ami=ami-067e015bbaa4e6726;instanceType=c6a.xlarge">us-east-2 (Ohio)</a> </li>
    </ul>
  </TabItem>
</Tabs>


### 2. EC2 Configuration
Pick the following appropriately for your instance. Make sure that this instance is reachable from the eBPF sensors running in your VPC.

1. Instance Name & tags
2. Key pair
3. The security group
   - Make sure to add rules to allow https traffic.
   - Allow UDP port 4789 if you are using traffic mirroring.
4. Disk storage. Choose at least 40GB

### 3. Add User Metadata to the EC2 instance
Under Advanced details > User Data, add the following (pick the appropriate value of `levo_auth_key`):

```bash
#!/bin/bash
echo "LEVOAI_AUTH_KEY=xxx" > /opt/levoai/.levoenv
sudo /opt/levoai/levo_satellite.sh start >> satellite-start.log 2>&1
# Uncomment the following line to enable the traffic mirroring listener
# sudo /opt/levoai/levo_traffic_listener.sh start >> traffic-listener-start.log 2>&1
```

#### Traffic Mirroring

In order to use traffic mirroring setup uncomment the last line of the user data script.
Check [Other Installation Options](other-installation-options) for configuring traffic mirroring using Levo CLI.


### 4. Launch the EC2 instance
Satellite services should start automatically once the EC2 instance is initialized

### 5. Verify the Satellite services
To check logs, debug and manage the Satellite services, you can SSH into the VM and use the following commands.
1. Stop the Satellite: `sudo /opt/levo/levo_satellite.sh stop`
2. Start the Satellite: `sudo /opt/levo/levo_satellite.sh start`
3. Upgrade the Satellite: `sudo /opt/levo/levo_satellite.sh upgrade`
4. Check the services: `sudo docker ps`

### 6. Verify connectivity with Levo.ai

#### a. Check Satellite health

The Satellite is comprised of four sub components 1) levoai-collector, 2) levoai-rabbitmq, 3)levoai-satellite, and 4) levoai-tagger.

Wait couple of minutes after the install, and check the health of the components by executing the following:

```bash
sudo docker ps -f name=levoai
```

If the Satellite is healthy, you should see output similar to below.

```bash
CONTAINER ID   IMAGE                     COMMAND                  CREATED             STATUS                  PORTS                                                                                                                                    NAMES
2b32cd6b9ced   levoai/collector:stable   "/usr/local/bin/levo…"   10 seconds ago      Up 8 seconds            0.0.0.0:4317->4317/tcp, 9411/tcp                                                                                                         levoai-collector
06f3c597cad0   levoai/satellite:stable   "gunicorn --capture-…"   10 seconds ago      Up 9 seconds            0.0.0.0:9999->9999/tcp                                                                                                                   levoai-satellite
89026034c567   levoai/satellite:stable   "python -OO /opt/lev…"   10 seconds ago      Up Less than a second                                                                                                                                            levoai-tagger
f74524d02fbd   bitnami/rabbitmq:3.10     "/opt/bitnami/script…"   10 seconds ago      Up 9 seconds            5551-5552/tcp, 0.0.0.0:4369->4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 0.0.0.0:15672->15672/tcp, 0.0.0.0:25672->25672/tcp, 15671/tcp   levoai-rabbitmq
```

#### b. Check connectivity
Execute the following to check for connectivity health:

```bash
sudo docker logs levoai-tagger  2>&1 | grep "Ready to process; waiting for messages."
```
If connectivity is healthy, you will see output similar to below.

```bash
{"level": "info", "time": "2022-06-07 08:07:22,439", "line": "rabbitmq_client.py:155", "version": "fc628b50354bf94e544eef46751d44945a2c55bc", "module": "/opt/levoai/e7s/src/python/levoai_e7s/satellite/rabbitmq_client.py", "message": "Ready to process; waiting for messages."}
```

### 7. Note down `Host:Port` information
The Collector now runs in a container, and is reachable on the host via port 4317 (on all the host's network interfaces).

Please note down the either the host's IP address or domain name. The Sensor will be configured to communicate with the Collector at <Host's IP|Domain-Name>:4317.

Please proceed to install the Sensor.

<br></br>

---

## Install in AWS EKS using EC2



### Prerequisites
- [eksctl](https://eksctl.io/) version >= `v0.152.0`
- [Helm v3](https://helm.sh/docs/intro/install/) installed and working.
- An AWS account with EKS permissions.
- At least 4 CPUs
- At least 8 GB RAM

### 1. Setup environment variables

```bash
export LEVOAI_AUTH_KEY=<'Authorization Key' copied earlier> 
CLUSTER_NAME=< INSERT CLUSTER NAME >
REGION=< INSERT AWS REGION >
ACCOUNT_ID=<INSERT AWS ACCOUNT ID>
```


### 2. Cluster Creation
```bash
read -r -d '' EKS_CLUSTER <<EOF
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: ${CLUSTER_NAME}
  region: ${AWS_REGION}

vpc:
  subnets:
    private:
      us-west-2a: { id: subnet-0d09e999a579e86ba }
      us-west-2b: { id: subnet-038ae44660fbc358b }
      us-west-2c: { id: subnet-06f1946a447e569d1 }

nodeGroups:
  - name: ng-e2e
    instanceType: t2.xlarge
    desiredCapacity: 1
    volumeSize: 40
    privateNetworking: true
EOF

echo "${EKS_CLUSTER}" > eks-cluster.yaml

eksctl create cluster -f ./configuration/eks-cluster.yaml

rm eks-cluster.yaml
```


### 3. Connecting to the cluster

AWS EKS grants cluster admin permissions to the account from which the cluster is created. If you don't need access to the cluster for other AWS Users, you can skip this section.

Access to other AWS users in the same account can be granted via 2 ways.
- [Adding individual access to user accounts](#adding-individuals-to-the-cluster)
- [Giving the permissions to a user group](#giving-access-to-an-iam-user-group)

#### Adding individuals to the cluster

This command can be run to add an inidividual user account to the cluster's aws-auth configmap

```bash
eksctl create iamidentitymapping \
    --cluster ${CLUSTER_NAME} \
    --region ${AWS_REGION} \
    --arn <AWS ACCOUNT ARN FOR THE USER> \
    --group system:masters \
    --no-duplicate-arns \
    --username <AWS USERNAME FOR THE USER>
```

#### Giving access to an IAM User Group 

We create a role developer.assume-access.role and attach two policies to it. The first one is `EKSFullAccess` so that it has access to all the EKS resources. The second one is `developer.assume-eks-access-role.policy` that allows assuming the role.

A detailed guide on defining the roles and policies can be found [here](https://eng.grip.security/enabling-aws-iam-group-access-to-an-eks-cluster-using-rbac).

Once you have followed the above guide to create the roles and attach the specific policies, you can add the role to the cluster's aws-auth config map to let the developers group access the cluster
```bash
eksctl create iamidentitymapping \
    --cluster ${CLUSTER_NAME} \
    --region ${AWS_REGION} \
    --arn arn:aws:iam::${ACCOUND_ID}:role/developer.assume-access.role \
    --group system:masters \
```

This needs to be run in order to grant access to the cluster.

One can Connect to the cluster by running just a single command

```bash
aws eks update-kubeconfig --name ${CLUSTER_NAME} --region ${AWS_REGION}> --role-arn arn:aws:iam::${ACCOUNT_ID}:role/developer.assume-access.role
```

This commands updates the kubeconfig and adds the context for the cluster and sets the current context to it.
The `--role` argument sets the correct role and policies so that seemless access to the cluster is granted instantly.


### 4. Setting the cluster up

#### Creating an OIDC provider

Run these two commands:
```bash
oidc_id=$(aws eks describe-cluster --name ${CLUSTER_NAME} --region ${REGION} --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
```

```bash
aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4 | cut -d "\"" -f1
```

If this returns a value, that is the OIDC ID that we need. If the statement returns nothing, run this command:
```bash
eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER_NAME} --region ${REGION} --approve
```

This creates an OIDC Identity Provider.

Next, to create a role in AWS for the EBS CSI Driver add-on ([Amazon Elastic Block Store CSI Driver](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) manages persistent volumes in EKS) we need to run these:

```bash
OIDC=$(aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4 | cut -d "\"" -f1)

read -r -d '' EBS_DRIVER_POLICY <<EOF
{
"Version": "2012-10-17",
"Statement": [
    {
    "Sid": "",
    "Effect": "Allow",
    "Principal": {
        "Federated": "arn:aws:iam::${ACCOUNT_ID}:oidc-provider/oidc.eks.${REGION}.amazonaws.com/id/${OIDC}"
    },
    "Action": "sts:AssumeRoleWithWebIdentity",
    "Condition": {
        "StringEquals": {
        "oidc.eks.${REGION}.amazonaws.com/id/${OIDC}:aud": "sts.amazonaws.com",
        "oidc.eks.${REGION}.amazonaws.com/id/${OIDC}:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa"
        }
    }
    }
]
}
EOF
echo "${EBS_DRIVER_POLICY}" > aws-ebs-csi-driver-trust-policy.json

aws iam create-role \
  --role-name AmazonEKS_EBS_CSI_DriverRole \
  --assume-role-policy-document file://aws-ebs-csi-driver-trust-policy.json

aws iam attach-role-policy \
  --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
  --role-name AmazonEKS_EBS_CSI_DriverRole

eksctl create addon --name aws-ebs-csi-driver --cluster ${CLUSTER_NAME} --region ${REGION} --service-account-role-arn arn:aws:iam::${ACCOUNT_ID}:role/AmazonEKS_EBS_CSI_DriverRole —force

rm aws-ebs-csi-driver-trust-policy.json
```
### 5. Install the satellite

To install the satellite, first add our helm repository
```bash
helm repo add levoai https://charts.levo.ai && helm repo update
```

Next, install the satellite from its helm chart

```bash
helm upgrade --install -n levoai --create-namespace \
  --set global.levoai_config_override.onprem-api.refresh-token=$LEVOAI_AUTH_KEY \
  levoai-satellite levoai/levoai-satellite
```


#### a. Check Satellite health

The Satellite is comprised of five sub components 1) levoai-collector, 2) levoai-ion, 3) levoai-rabbitmq, 4)levoai-satellite, and 5) levoai-tagger.

Wait couple of minutes after the install, and check the health of the components by executing the following:

```bash
kubectl -n levoai get pods
```                              
If the Satellite is healthy, you should see output similar to below. Don't worry about the restarts of the levoai-tagger pod.

```bash
NAME                                READY   STATUS    RESTARTS      AGE
levoai-collector-848fb4fff9-gv8g9   1/1     Running   0             4m8s
levoai-ion-65686bd9c6-k2vgc         1/1     Running   0             4m8s
levoai-rabbitmq-0                   1/1     Running   0             4m8s
levoai-satellite-54956ccb89-5s4h2   1/1     Running   0             4m8s
levoai-tagger-799db4d9cc-89jm8      1/1     Running   3 (4m8s ago)  4m8s
```

#### b. Check connectivity
Execute the following to check for connectivity health:

```bash
# Please specify the actual pod name for levoai-tagger below
kubectl -n levoai logs <levoai-tagger pod name> | grep "Ready to process; waiting for messages."
```
If connectivity is healthy, you will see output similar to below.

```bash
{"level": "info", "time": "2022-06-07 08:07:22,439", "line": "rabbitmq_client.py:155", "version": "fc628b50354bf94e544eef46751d44945a2c55bc", "module": "/opt/levoai/e7s/src/python/levoai_e7s/satellite/rabbitmq_client.py", "message": "Ready to process; waiting for messages."}
```

**Please contact `support@levo.ai` if you notice health/connectivity related errors.**
<br></br>

-------------------------------------------------